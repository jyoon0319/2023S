{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyoon0319/2023S/blob/main/NLTK_POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6i0QbOc0m-f"
      },
      "source": [
        "# Parts-of-Speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5Bg6Wo0m-o"
      },
      "source": [
        "## https://medium.com/codex/linguistic-modelling-techniques-with-python-de3baf4bb752"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import of nltk\n",
        "import nltk\n",
        "# some further components for segementation, tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# download the universal tagset\n",
        "nltk.download('universal_tagset')\n",
        "# import the word_tokenize class\n",
        "from nltk.tokenize import word_tokenize\n",
        "print('\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfXJlq4aiK4u",
        "outputId": "19918d69-caab-4cb7-8b41-eb0a4fcf4a3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the word-tokenizer to a text string and find the POS tag\n",
        "token = (\"In the present study, we examine the outcomes of such a period of no exposure on the neurocognition of L2 grammar:\")\n",
        "result = nltk.pos_tag(word_tokenize(token), tagset='universal')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9IDIkxvoskv",
        "outputId": "d6b43bb5-1185-4878-9cbf-3bb58e378f2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('In', 'ADP'), ('the', 'DET'), ('present', 'ADJ'), ('study', 'NOUN'), (',', '.'), ('we', 'PRON'), ('examine', 'VERB'), ('the', 'DET'), ('outcomes', 'NOUN'), ('of', 'ADP'), ('such', 'ADJ'), ('a', 'DET'), ('period', 'NOUN'), ('of', 'ADP'), ('no', 'DET'), ('exposure', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('neurocognition', 'NOUN'), ('of', 'ADP'), ('L2', 'NOUN'), ('grammar', 'NOUN'), (':', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## select word tokens with specific POS"
      ],
      "metadata": {
        "id": "Ric8MJvLsCKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tag_set = ['NOUN', 'VERB', 'ADJ']\n",
        "my_words = [word for word, tag in result if tag in my_tag_set]\n",
        "print(my_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi5Q5goFmDrU",
        "outputId": "109e5cba-590a-476c-cd72-dde8c69ed8da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['present', 'study', 'examine', 'outcomes', 'such', 'period', 'exposure', 'neurocognition', 'L2', 'grammar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS List in NLPK: https://dbrang.tistory.com/1139"
      ],
      "metadata": {
        "id": "56pGH0wvkGm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's change the tagset\n",
        "token = (\"In the present study, we examine the outcomes of such a period of no exposure on the neurocognition of L2 grammar:\")\n",
        "result = nltk.pos_tag(word_tokenize(token))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZGL8BfYrvVu",
        "outputId": "7be7ae8e-4c7b-4e75-f05e-8450600ccc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('In', 'IN'), ('the', 'DT'), ('present', 'JJ'), ('study', 'NN'), (',', ','), ('we', 'PRP'), ('examine', 'VBP'), ('the', 'DT'), ('outcomes', 'NNS'), ('of', 'IN'), ('such', 'JJ'), ('a', 'DT'), ('period', 'NN'), ('of', 'IN'), ('no', 'DT'), ('exposure', 'NN'), ('on', 'IN'), ('the', 'DT'), ('neurocognition', 'NN'), ('of', 'IN'), ('L2', 'NNP'), ('grammar', 'NN'), (':', ':')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Practice POS tagging] Exercise 2.23 Prepositions (section 2.41)\n",
        "Underline the prepositions in this extract.\n",
        "\n",
        "Malaysia Airlines Flight 370 disappeared on 8 March 2014, while flying from Kuala Lumpur to Beijing. Air traffic control lost contact with the plane when it was over the South China Sea. Neither the crew nor the aircraft’s\n",
        "communication systems relayed any distress signal, indications of bad weather or technical problems. The aircraft had 12 Malaysian crew members and 227 passengers on board. A major search operation began in the Gulf of\n",
        "Thailand and was then extended to the Strait of Malacca and the Andaman Sea. Analysis of satellite communications showed that the flight continued until 8:19 local time. It then flew south towards the southern Indian Ocean. During several dramatic days, the Malaysian search team worked closely with foreign aviation experts. Then, on 17 March, Australia took charge of the search and the focus shifted to the southern Indian Ocean. The latest phase of the search is concentrating on the seafloor southwest of Perth, Australia. Despite an extensive search over vast distances, no debris has been found and we seem to be no nearer to a solution to this mystery. To date, the search for MH370 is the most expensive search in aviation history"
      ],
      "metadata": {
        "id": "CD6g14-fite8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the word-tokenizer to a text string and find the POS tag\n",
        "token = (\"Malaysia Airlines Flight 370 disappeared on 8 March 2014, while flying from Kuala Lumpur to Beijing. Air traffic control lost contact with the plane when it was over the South China Sea. Neither the crew nor the aircraft’s communication systems relayed any distress signal, indications of bad weather or technical problems. The aircraft had 12 Malaysian crew members and 227 passengers on board. A major search operation began in the Gulf of Thailand and was then extended to the Strait of Malacca and the Andaman Sea. Analysis of satellite communications showed that the flight continued until 8:19 local time. It then flew south towards the southern Indian Ocean. During several dramatic days, the Malaysian search team worked closely with foreign aviation experts. Then, on 17 March, Australia took charge of the search and the focus shifted to the southern Indian Ocean. The latest phase of the search is concentrating on the seafloor southwest of Perth, Australia. Despite an extensive search over vast distances, no debris has been found and we seem to be no nearer to a solution to this mystery. To date, the search for MH370 is the most expensive search in aviation history\")\n",
        "result = nltk.pos_tag(word_tokenize(token), tagset='universal')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "QhCICjypqv20",
        "outputId": "68ef5f2b-59dd-46f1-b47b-8f9ccf286e2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Malaysia', 'NOUN'), ('Airlines', 'NOUN'), ('Flight', 'NOUN'), ('370', 'NUM'), ('disappeared', 'VERB'), ('on', 'ADP'), ('8', 'NUM'), ('March', 'NOUN'), ('2014', 'NUM'), (',', '.'), ('while', 'ADP'), ('flying', 'VERB'), ('from', 'ADP'), ('Kuala', 'NOUN'), ('Lumpur', 'NOUN'), ('to', 'PRT'), ('Beijing', 'NOUN'), ('.', '.'), ('Air', 'NOUN'), ('traffic', 'NOUN'), ('control', 'NOUN'), ('lost', 'VERB'), ('contact', 'NOUN'), ('with', 'ADP'), ('the', 'DET'), ('plane', 'NOUN'), ('when', 'ADV'), ('it', 'PRON'), ('was', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('South', 'NOUN'), ('China', 'NOUN'), ('Sea', 'NOUN'), ('.', '.'), ('Neither', 'CONJ'), ('the', 'DET'), ('crew', 'NOUN'), ('nor', 'CONJ'), ('the', 'DET'), ('aircraft', 'NOUN'), ('’', 'NOUN'), ('s', 'ADJ'), ('communication', 'NOUN'), ('systems', 'NOUN'), ('relayed', 'VERB'), ('any', 'DET'), ('distress', 'ADJ'), ('signal', 'NOUN'), (',', '.'), ('indications', 'NOUN'), ('of', 'ADP'), ('bad', 'ADJ'), ('weather', 'NOUN'), ('or', 'CONJ'), ('technical', 'ADJ'), ('problems', 'NOUN'), ('.', '.'), ('The', 'DET'), ('aircraft', 'NOUN'), ('had', 'VERB'), ('12', 'NUM'), ('Malaysian', 'ADJ'), ('crew', 'NOUN'), ('members', 'NOUN'), ('and', 'CONJ'), ('227', 'NUM'), ('passengers', 'NOUN'), ('on', 'ADP'), ('board', 'NOUN'), ('.', '.'), ('A', 'DET'), ('major', 'ADJ'), ('search', 'NOUN'), ('operation', 'NOUN'), ('began', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('Gulf', 'NOUN'), ('of', 'ADP'), ('Thailand', 'NOUN'), ('and', 'CONJ'), ('was', 'VERB'), ('then', 'ADV'), ('extended', 'VERB'), ('to', 'PRT'), ('the', 'DET'), ('Strait', 'NOUN'), ('of', 'ADP'), ('Malacca', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Andaman', 'NOUN'), ('Sea', 'NOUN'), ('.', '.'), ('Analysis', 'NOUN'), ('of', 'ADP'), ('satellite', 'NOUN'), ('communications', 'NOUN'), ('showed', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('flight', 'NOUN'), ('continued', 'VERB'), ('until', 'ADP'), ('8:19', 'NUM'), ('local', 'ADJ'), ('time', 'NOUN'), ('.', '.'), ('It', 'PRON'), ('then', 'ADV'), ('flew', 'VERB'), ('south', 'ADJ'), ('towards', 'ADP'), ('the', 'DET'), ('southern', 'ADJ'), ('Indian', 'ADJ'), ('Ocean', 'NOUN'), ('.', '.'), ('During', 'ADP'), ('several', 'ADJ'), ('dramatic', 'ADJ'), ('days', 'NOUN'), (',', '.'), ('the', 'DET'), ('Malaysian', 'ADJ'), ('search', 'NOUN'), ('team', 'NOUN'), ('worked', 'VERB'), ('closely', 'ADV'), ('with', 'ADP'), ('foreign', 'ADJ'), ('aviation', 'NOUN'), ('experts', 'NOUN'), ('.', '.'), ('Then', 'ADV'), (',', '.'), ('on', 'ADP'), ('17', 'NUM'), ('March', 'NOUN'), (',', '.'), ('Australia', 'NOUN'), ('took', 'VERB'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('search', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('focus', 'NOUN'), ('shifted', 'VERB'), ('to', 'PRT'), ('the', 'DET'), ('southern', 'ADJ'), ('Indian', 'ADJ'), ('Ocean', 'NOUN'), ('.', '.'), ('The', 'DET'), ('latest', 'ADJ'), ('phase', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('search', 'NOUN'), ('is', 'VERB'), ('concentrating', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('seafloor', 'ADJ'), ('southwest', 'NOUN'), ('of', 'ADP'), ('Perth', 'NOUN'), (',', '.'), ('Australia', 'NOUN'), ('.', '.'), ('Despite', 'ADP'), ('an', 'DET'), ('extensive', 'ADJ'), ('search', 'NOUN'), ('over', 'ADP'), ('vast', 'ADJ'), ('distances', 'NOUN'), (',', '.'), ('no', 'DET'), ('debris', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('found', 'VERB'), ('and', 'CONJ'), ('we', 'PRON'), ('seem', 'VERB'), ('to', 'PRT'), ('be', 'VERB'), ('no', 'DET'), ('nearer', 'NOUN'), ('to', 'PRT'), ('a', 'DET'), ('solution', 'NOUN'), ('to', 'PRT'), ('this', 'DET'), ('mystery', 'NOUN'), ('.', '.'), ('To', 'PRT'), ('date', 'NOUN'), (',', '.'), ('the', 'DET'), ('search', 'NOUN'), ('for', 'ADP'), ('MH370', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('most', 'ADV'), ('expensive', 'ADJ'), ('search', 'NOUN'), ('in', 'ADP'), ('aviation', 'NOUN'), ('history', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tag_set = ['ADP']\n",
        "my_words = [word for word, tag in result if tag in my_tag_set]\n",
        "print(my_words)"
      ],
      "metadata": {
        "id": "I6cojIjzsdRx",
        "outputId": "13d0b7b6-aff2-4e48-fb43-be4af57f87e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['on', 'while', 'from', 'with', 'over', 'of', 'on', 'in', 'of', 'of', 'of', 'that', 'until', 'towards', 'During', 'with', 'on', 'of', 'of', 'on', 'of', 'Despite', 'over', 'for', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Practice] Exercise 2.22 Conjunctions (sections 2.39–2.40)\n",
        "Circle the conjunctions in the following sentences and decide whether they are coordinators (C) or subordinators\n",
        "(S)."
      ],
      "metadata": {
        "id": "YdGnkszYsrSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = input(\"Type the conjunction in 1.In an age of specialization, branding and market segmentation, everybody in the banking business wants to appear distinct from everybody else:\" )\n",
        "a1 = input(\"Is it C or S?\" )\n",
        "b = input(\"Type the conjunction in 2. Many banks are too small to harbour international ambitions but domestically they want to be in every high street..:\" )\n",
        "b1 = input(\"Is it C or S?\" )\n",
        "c = input(\"Type the conjunction in 3. Some smaller banks began as local lending institutions in the nineteenth century, when middle-class incomes soared.:\" )\n",
        "c1 = input(\"Is it C or S?\" )\n",
        "d = input(\"Type the conjunction in 4:\")\n",
        "d1 = input(\"Is it C or S?\" )\n",
        "e = input(\"Type the conjunction in 5:\")\n",
        "e1 = input(\"Is it C or S?\" )\n",
        "f = input(\"Type the conjunction in 6:\")\n",
        "f1 = input(\"Is it C or S?\" )\n",
        "g = input(\"Type the conjunction in 7:\")\n",
        "g1 = input(\"Is it C or S?\" )\n",
        "h = input(\"Type the conjunction in 8:\")\n",
        "h1 = input(\"Is it C or S?\" )\n",
        "\n",
        "\n",
        "print(a, 'in (1) is', a1)\n",
        "print(b, 'in (2) is', b1)\n",
        "print(c, 'in (3) is', c1)\n",
        "print(d, 'in (4) is', d1)\n",
        "print(e, 'in (4) is', e1)\n",
        "print(f, 'in (4) is', f1)\n",
        "print(g, 'in (4) is', g1)\n",
        "print(h, 'in (4) is', h1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "Nx9ia44ateoY",
        "outputId": "aa0bbfbe-3179-4181-eb6a-b436c00eed43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type the conjunction in 1.In an age of specialization, branding and market segmentation, everybody in the banking business wants to appear distinct from everybody else:and\n",
            "Is it C or S?C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c9033dd9f07d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type the conjunction in 1.In an age of specialization, branding and market segmentation, everybody in the banking business wants to appear distinct from everybody else:\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Is it C or S?\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type the conjunction in 2. Many banks are too small to harbour international ambitions but domestically they want to be in every high street..:\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Is it C or S?\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type the conjunction in 3. Some smaller banks began as local lending institutions in the nineteenth century, when middle-class incomes soared.:\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's ask NLTK"
      ],
      "metadata": {
        "id": "C0bqIwKRwb5W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxlb6esWxUDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 더 보기: 텍스트 전처리 \n",
        "[파이썬 텍스트 마이닝 완벽 가이드 > 예제코드 다운로드 > Chapter 2]\n",
        "downloadable from https://wikibook.co.kr/textmining/"
      ],
      "metadata": {
        "id": "-f3vpunnqBV3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ6vSHne0m-q"
      },
      "source": [
        "### 2.1 문장 토큰화(sentence tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQl5aO2f0m-q"
      },
      "outputs": [],
      "source": [
        "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n4fJHz_0m-q",
        "outputId": "0d214818-8352-467f-cbc8-5c56f8fb00f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(para)) #주어진 text를 sentence 단위로 tokenize함. 주로 . ! ? 등을 이용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3DAvybm0m-r",
        "outputId": "93fceffe-0093-477f-d74b-7a423dee9068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
          ]
        }
      ],
      "source": [
        "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
        "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
        "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
        "\n",
        "import nltk.data\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
        "print(tokenizer.tokenize(paragraph_french))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs6i3L5Q0m-r"
      },
      "outputs": [],
      "source": [
        "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCJ2dRtf0m-r",
        "outputId": "5bdfd903-54b1-4618-a35e-30a6bf483976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
          ]
        }
      ],
      "source": [
        "print(sent_tokenize(para_kor)) #한국어에 대해서도 sentence tokenizer는 잘 동작함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdGVGT4A0m-r"
      },
      "source": [
        "### 2.2 단어 토큰화 (word tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx9JTNW_0m-s",
        "outputId": "ac6c59d5-7f32-4f6a-a6c7-ed925f8de9ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(para)) #주어진 text를 word 단위로 tokenize함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM3vI6yS0m-s",
        "outputId": "4cdea833-85c2-4b84-d23f-73753700ac52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer  \n",
        "print(WordPunctTokenizer().tokenize(para))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFj5TNZ90m-s",
        "outputId": "6b9975d1-d847-4080-d0ba-792a1178ef18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
          ]
        }
      ],
      "source": [
        "print(word_tokenize(para_kor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCa3h8xH0m-u"
      },
      "source": [
        "### 2.4 노이즈와 불용어 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBcHo6Dp0m-v",
        "outputId": "6d532779-fb8f-489b-af50-dab79ca7d4a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sorry', 'go', 'movie', 'yesterday']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들\n",
        "english_stops = set(stopwords.words('english')) #반복이 되지 않도록 set으로 변환\n",
        "\n",
        "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "tokens = tokenizer.tokenize(text1.lower()) #word_tokenize로 토큰화\n",
        "\n",
        "result = [word for word in tokens if word not in english_stops] #stopwords를 제외한 단어들만으로 list를 생성\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nKFkppx0m-v",
        "outputId": "77594f86-24f8-4c4b-9a8b-c55402139d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'is', \"needn't\", 'couldn', 'o', 'through', \"didn't\", 'up', \"won't\", \"weren't\", 'they', 'wasn', \"shan't\", 'off', 'above', 'if', 'his', 'or', 'their', 'it', 'yours', 'on', 'as', 'for', 'during', 'yourself', 'am', 'because', 'before', 'theirs', 'be', \"mustn't\", 'and', 'such', 'an', 'here', 'to', \"couldn't\", \"doesn't\", 'hers', 've', 'do', 'did', 'ours', 'herself', 'isn', 'by', 'yourselves', 'had', 'were', 'll', 'her', 'until', 'those', 'ma', 'myself', 'between', 'with', 'doesn', \"shouldn't\", 'him', 'in', 'while', 'own', 'this', 'we', 'having', 'out', \"hadn't\", 'all', 'won', 'there', 'm', 'same', \"wasn't\", 'our', 'not', 'any', 'what', 'does', \"you're\", 'then', 'who', 'these', 'once', 'now', \"hasn't\", 'shan', 'them', 'further', 'has', \"don't\", 'don', 'your', 'few', 'd', 'didn', 'into', 'at', \"you've\", 'below', 'himself', 'been', \"wouldn't\", 'some', 'how', 'more', 'each', 'ain', \"she's\", 'where', 'only', 'have', 'the', 're', 'needn', 'weren', 'are', 'whom', 'haven', 'most', 'me', \"that'll\", 'under', 's', 'wouldn', 't', 'can', 'mightn', 'you', 'against', 'too', 'from', 'my', 'just', 'that', \"you'll\", 'again', 'hasn', 'itself', 'a', 'doing', 'was', \"aren't\", \"it's\", \"haven't\", 'should', \"isn't\", 'its', 'aren', 'down', 'i', 'but', 'when', 'no', \"you'd\", 'themselves', 'over', 'mustn', 'about', 'being', 'so', \"should've\", 'ourselves', 'why', 'he', 'other', 'nor', 'of', 'she', 'after', 'hadn', 'will', 'y', 'shouldn', 'very', 'which', 'than', \"mightn't\", 'both'}\n"
          ]
        }
      ],
      "source": [
        "print(english_stops) #nltk가 제공하는 영어 stopword를 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM40943A0m-v",
        "outputId": "2715e818-9b4f-468d-8b07-90127b64f718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
          ]
        }
      ],
      "source": [
        "#자신만의 stopwords를 만들고 이용\n",
        "#한글처리에서도 유용하게 사용할 수 있음\n",
        "my_stopword = ['i', 'go', 'to'] #나만의 stopword를 리스트로 정의\n",
        "result = [word for word in tokens if word not in my_stopword] \n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKZHUcl90m-x"
      },
      "source": [
        "# 3. 품사 태깅(Part-of-Speech Tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_yULseB0m-x"
      },
      "source": [
        "## 3.1 품사의 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mL7LOC40m-x"
      },
      "source": [
        "## 3.2 NLTK를 이용한 품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_mMq3ao0m-x",
        "outputId": "de32950b-43ee-4a4a-c18e-9b1f5bc0c5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Malaysia', 'NNP'), ('Airlines', 'NNPS'), ('Flight', 'NNP'), ('370', 'CD'), ('disappeared', 'VBD'), ('on', 'IN'), ('8', 'CD'), ('March', 'NNP'), ('2014', 'CD'), (',', ','), ('while', 'IN'), ('flying', 'VBG'), ('from', 'IN'), ('Kuala', 'NNP'), ('Lumpur', 'NNP'), ('to', 'TO'), ('Beijing', 'NNP'), ('.', '.'), ('Air', 'NNP'), ('traffic', 'NN'), ('control', 'NN'), ('lost', 'VBN'), ('contact', 'NN'), ('with', 'IN'), ('the', 'DT'), ('plane', 'NN'), ('when', 'WRB'), ('it', 'PRP'), ('was', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('South', 'NNP'), ('China', 'NNP'), ('Sea', 'NNP'), ('.', '.'), ('Neither', 'CC'), ('the', 'DT'), ('crew', 'NN'), ('nor', 'CC'), ('the', 'DT'), ('aircraft', 'NN'), ('’', 'NNP'), ('s', 'JJ'), ('communication', 'NN'), ('systems', 'NNS'), ('relayed', 'VBD'), ('any', 'DT'), ('distress', 'JJ'), ('signal', 'NN'), (',', ','), ('indications', 'NNS'), ('of', 'IN'), ('bad', 'JJ'), ('weather', 'NN'), ('or', 'CC'), ('technical', 'JJ'), ('problems', 'NNS'), ('.', '.'), ('The', 'DT'), ('aircraft', 'NN'), ('had', 'VBD'), ('12', 'CD'), ('Malaysian', 'JJ'), ('crew', 'NN'), ('members', 'NNS'), ('and', 'CC'), ('227', 'CD'), ('passengers', 'NNS'), ('on', 'IN'), ('board', 'NN'), ('.', '.'), ('A', 'DT'), ('major', 'JJ'), ('search', 'NN'), ('operation', 'NN'), ('began', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('Gulf', 'NNP'), ('of', 'IN'), ('Thailand', 'NNP'), ('and', 'CC'), ('was', 'VBD'), ('then', 'RB'), ('extended', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('Strait', 'NNP'), ('of', 'IN'), ('Malacca', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('Andaman', 'NNP'), ('Sea', 'NNP'), ('.', '.'), ('Analysis', 'NN'), ('of', 'IN'), ('satellite', 'NN'), ('communications', 'NNS'), ('showed', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('flight', 'NN'), ('continued', 'VBD'), ('until', 'IN'), ('8:19', 'CD'), ('local', 'JJ'), ('time', 'NN'), ('.', '.'), ('It', 'PRP'), ('then', 'RB'), ('flew', 'VBD'), ('south', 'JJ'), ('towards', 'IN'), ('the', 'DT'), ('southern', 'JJ'), ('Indian', 'JJ'), ('Ocean', 'NNP'), ('.', '.'), ('During', 'IN'), ('several', 'JJ'), ('dramatic', 'JJ'), ('days', 'NNS'), (',', ','), ('the', 'DT'), ('Malaysian', 'JJ'), ('search', 'NN'), ('team', 'NN'), ('worked', 'VBD'), ('closely', 'RB'), ('with', 'IN'), ('foreign', 'JJ'), ('aviation', 'NN'), ('experts', 'NNS'), ('.', '.'), ('Then', 'RB'), (',', ','), ('on', 'IN'), ('17', 'CD'), ('March', 'NNP'), (',', ','), ('Australia', 'NNP'), ('took', 'VBD'), ('charge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('search', 'NN'), ('and', 'CC'), ('the', 'DT'), ('focus', 'NN'), ('shifted', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('southern', 'JJ'), ('Indian', 'JJ'), ('Ocean', 'NNP'), ('.', '.'), ('The', 'DT'), ('latest', 'JJS'), ('phase', 'NN'), ('of', 'IN'), ('the', 'DT'), ('search', 'NN'), ('is', 'VBZ'), ('concentrating', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('seafloor', 'JJ'), ('southwest', 'NN'), ('of', 'IN'), ('Perth', 'NNP'), (',', ','), ('Australia', 'NNP'), ('.', '.'), ('Despite', 'IN'), ('an', 'DT'), ('extensive', 'JJ'), ('search', 'NN'), ('over', 'IN'), ('vast', 'JJ'), ('distances', 'NNS'), (',', ','), ('no', 'DT'), ('debris', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('found', 'VBN'), ('and', 'CC'), ('we', 'PRP'), ('seem', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('no', 'DT'), ('nearer', 'NN'), ('to', 'TO'), ('a', 'DT'), ('solution', 'NN'), ('to', 'TO'), ('this', 'DT'), ('mystery', 'NN'), ('.', '.'), ('To', 'TO'), ('date', 'NN'), (',', ','), ('the', 'DT'), ('search', 'NN'), ('for', 'IN'), ('MH370', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('most', 'RBS'), ('expensive', 'JJ'), ('search', 'NN'), ('in', 'IN'), ('aviation', 'NN'), ('history', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(\"Malaysia Airlines Flight 370 disappeared on 8 March 2014, while flying from Kuala Lumpur to Beijing. Air traffic control lost contact with the plane when it was over the South China Sea. Neither the crew nor the aircraft’s communication systems relayed any distress signal, indications of bad weather or technical problems. The aircraft had 12 Malaysian crew members and 227 passengers on board. A major search operation began in the Gulf of Thailand and was then extended to the Strait of Malacca and the Andaman Sea. Analysis of satellite communications showed that the flight continued until 8:19 local time. It then flew south towards the southern Indian Ocean. During several dramatic days, the Malaysian search team worked closely with foreign aviation experts. Then, on 17 March, Australia took charge of the search and the focus shifted to the southern Indian Ocean. The latest phase of the search is concentrating on the seafloor southwest of Perth, Australia. Despite an extensive search over vast distances, no debris has been found and we seem to be no nearer to a solution to this mystery. To date, the search for MH370 is the most expensive search in aviation history\")\n",
        "print(nltk.pos_tag(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP2ba5JJ0m-x",
        "outputId": "24a21b85-866d-44c5-b225-9886c46d0d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['traffic', 'control', 'contact', 'plane', 'crew', 'aircraft', 's', 'communication', 'distress', 'signal', 'bad', 'weather', 'technical', 'aircraft', 'Malaysian', 'crew', 'board', 'major', 'search', 'operation', 'Analysis', 'satellite', 'flight', 'local', 'time', 'south', 'southern', 'Indian', 'several', 'dramatic', 'Malaysian', 'search', 'team', 'foreign', 'aviation', 'charge', 'search', 'focus', 'southern', 'Indian', 'phase', 'search', 'seafloor', 'southwest', 'extensive', 'search', 'vast', 'debris', 'be', 'nearer', 'solution', 'mystery', 'date', 'search', 'expensive', 'search', 'aviation', 'history']\n"
          ]
        }
      ],
      "source": [
        "my_tag_set = ['NN', 'VB', 'JJ']\n",
        "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
        "print(my_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcgTFdkB0m-y",
        "outputId": "65194cee-2f0b-4e35-cda7-1ffe1be4d6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']\n"
          ]
        }
      ],
      "source": [
        "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\n",
        "print(words_with_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5YYSENi0m-y"
      },
      "source": [
        "## 3.3 한글 형태소 분석과 품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnSWXAHX0m-y"
      },
      "outputs": [],
      "source": [
        "sentence = '''절망의 반대가 희망은 아니다.\n",
        "어두운 밤하늘에 별이 빛나듯\n",
        "희망은 절망 속에 싹트는 거지\n",
        "만약에 우리가 희망함이 적다면\n",
        "그 누가 세상을 비출어줄까.\n",
        "정희성, 희망 공부'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-H3GXY40m-y",
        "outputId": "48bcaf00-2180-4a0f-aaf9-8a01ce5746fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비출어줄까', '.', '정희성', ',', '희망', '공부']\n",
            "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비출어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)\n",
        "print(nltk.pos_tag(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx6KuJQR0m-y"
      },
      "source": [
        "### KoNLPy 설치\n",
        "\n",
        "https://konlpy.org/ko/latest/install/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPWGl_Aj0m-y"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Okt\n",
        "t = Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VXpdZy9-0m-z",
        "outputId": "b0f63618-c4a8-4a1a-c935-756a53812c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "형태소: ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
            "\n",
            "명사: ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n",
            "\n",
            "품사 태깅 결과: [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n"
          ]
        }
      ],
      "source": [
        "print('형태소:', t.morphs(sentence))\n",
        "print()\n",
        "print('명사:', t.nouns(sentence))\n",
        "print()\n",
        "print('품사 태깅 결과:', t.pos(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 연습문제 정답 "
      ],
      "metadata": {
        "id": "SSkxgm15rIJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = (\"Malaysia Airlines Flight 370 disappeared on 8 March 2014, while flying from Kuala Lumpur to Beijing. Air traffic control lost contact with the plane when it was over the South China Sea. Neither the crew nor the aircraft’s communication systems relayed any distress signal, indications of bad weather or technical problems. The aircraft had 12 Malaysian crew members and 227 passengers on board. A major search operation began in the Gulf of Thailand and was then extended to the Strait of Malacca and the Andaman Sea. Analysis of satellite communications showed that the flight continued until 8:19 local time. It then flew south towards the southern Indian Ocean. During several dramatic days, the Malaysian search team worked closely with foreign aviation experts. Then, on 17 March, Australia took charge of the search and the focus shifted to the southern Indian Ocean. The latest phase of the search is concentrating on the seafloor southwest of Perth, Australia. Despite an extensive search over vast distances, no debris has been found and we seem to be no nearer to a solution to this mystery. To date, the search for MH370 is the most expensive search in aviation history\")\n",
        "result = nltk.pos_tag(word_tokenize(token), tagset='universal')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jfDa6lUpGvc",
        "outputId": "57b33a19-f93d-44a8-b579-897d58ca6d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Malaysia', 'NOUN'), ('Airlines', 'NOUN'), ('Flight', 'NOUN'), ('370', 'NUM'), ('disappeared', 'VERB'), ('on', 'ADP'), ('8', 'NUM'), ('March', 'NOUN'), ('2014', 'NUM'), (',', '.'), ('while', 'ADP'), ('flying', 'VERB'), ('from', 'ADP'), ('Kuala', 'NOUN'), ('Lumpur', 'NOUN'), ('to', 'PRT'), ('Beijing', 'NOUN'), ('.', '.'), ('Air', 'NOUN'), ('traffic', 'NOUN'), ('control', 'NOUN'), ('lost', 'VERB'), ('contact', 'NOUN'), ('with', 'ADP'), ('the', 'DET'), ('plane', 'NOUN'), ('when', 'ADV'), ('it', 'PRON'), ('was', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('South', 'NOUN'), ('China', 'NOUN'), ('Sea', 'NOUN'), ('.', '.'), ('Neither', 'CONJ'), ('the', 'DET'), ('crew', 'NOUN'), ('nor', 'CONJ'), ('the', 'DET'), ('aircraft', 'NOUN'), ('’', 'NOUN'), ('s', 'ADJ'), ('communication', 'NOUN'), ('systems', 'NOUN'), ('relayed', 'VERB'), ('any', 'DET'), ('distress', 'ADJ'), ('signal', 'NOUN'), (',', '.'), ('indications', 'NOUN'), ('of', 'ADP'), ('bad', 'ADJ'), ('weather', 'NOUN'), ('or', 'CONJ'), ('technical', 'ADJ'), ('problems', 'NOUN'), ('.', '.'), ('The', 'DET'), ('aircraft', 'NOUN'), ('had', 'VERB'), ('12', 'NUM'), ('Malaysian', 'ADJ'), ('crew', 'NOUN'), ('members', 'NOUN'), ('and', 'CONJ'), ('227', 'NUM'), ('passengers', 'NOUN'), ('on', 'ADP'), ('board', 'NOUN'), ('.', '.'), ('A', 'DET'), ('major', 'ADJ'), ('search', 'NOUN'), ('operation', 'NOUN'), ('began', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('Gulf', 'NOUN'), ('of', 'ADP'), ('Thailand', 'NOUN'), ('and', 'CONJ'), ('was', 'VERB'), ('then', 'ADV'), ('extended', 'VERB'), ('to', 'PRT'), ('the', 'DET'), ('Strait', 'NOUN'), ('of', 'ADP'), ('Malacca', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Andaman', 'NOUN'), ('Sea', 'NOUN'), ('.', '.'), ('Analysis', 'NOUN'), ('of', 'ADP'), ('satellite', 'NOUN'), ('communications', 'NOUN'), ('showed', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('flight', 'NOUN'), ('continued', 'VERB'), ('until', 'ADP'), ('8:19', 'NUM'), ('local', 'ADJ'), ('time', 'NOUN'), ('.', '.'), ('It', 'PRON'), ('then', 'ADV'), ('flew', 'VERB'), ('south', 'ADJ'), ('towards', 'ADP'), ('the', 'DET'), ('southern', 'ADJ'), ('Indian', 'ADJ'), ('Ocean', 'NOUN'), ('.', '.'), ('During', 'ADP'), ('several', 'ADJ'), ('dramatic', 'ADJ'), ('days', 'NOUN'), (',', '.'), ('the', 'DET'), ('Malaysian', 'ADJ'), ('search', 'NOUN'), ('team', 'NOUN'), ('worked', 'VERB'), ('closely', 'ADV'), ('with', 'ADP'), ('foreign', 'ADJ'), ('aviation', 'NOUN'), ('experts', 'NOUN'), ('.', '.'), ('Then', 'ADV'), (',', '.'), ('on', 'ADP'), ('17', 'NUM'), ('March', 'NOUN'), (',', '.'), ('Australia', 'NOUN'), ('took', 'VERB'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('search', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('focus', 'NOUN'), ('shifted', 'VERB'), ('to', 'PRT'), ('the', 'DET'), ('southern', 'ADJ'), ('Indian', 'ADJ'), ('Ocean', 'NOUN'), ('.', '.'), ('The', 'DET'), ('latest', 'ADJ'), ('phase', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('search', 'NOUN'), ('is', 'VERB'), ('concentrating', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('seafloor', 'ADJ'), ('southwest', 'NOUN'), ('of', 'ADP'), ('Perth', 'NOUN'), (',', '.'), ('Australia', 'NOUN'), ('.', '.'), ('Despite', 'ADP'), ('an', 'DET'), ('extensive', 'ADJ'), ('search', 'NOUN'), ('over', 'ADP'), ('vast', 'ADJ'), ('distances', 'NOUN'), (',', '.'), ('no', 'DET'), ('debris', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('found', 'VERB'), ('and', 'CONJ'), ('we', 'PRON'), ('seem', 'VERB'), ('to', 'PRT'), ('be', 'VERB'), ('no', 'DET'), ('nearer', 'NOUN'), ('to', 'PRT'), ('a', 'DET'), ('solution', 'NOUN'), ('to', 'PRT'), ('this', 'DET'), ('mystery', 'NOUN'), ('.', '.'), ('To', 'PRT'), ('date', 'NOUN'), (',', '.'), ('the', 'DET'), ('search', 'NOUN'), ('for', 'ADP'), ('MH370', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('most', 'ADV'), ('expensive', 'ADJ'), ('search', 'NOUN'), ('in', 'ADP'), ('aviation', 'NOUN'), ('history', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tag_set = ['ADP']\n",
        "my_words = [word for word, tag in result if tag in my_tag_set]\n",
        "print(my_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gUEQ3iNpKCo",
        "outputId": "e2b64fb6-5396-4cc0-dc5f-a240777e16fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['on', 'while', 'from', 'with', 'over', 'of', 'on', 'in', 'of', 'of', 'of', 'that', 'until', 'towards', 'During', 'with', 'on', 'of', 'of', 'on', 'of', 'Despite', 'over', 'for', 'in']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}